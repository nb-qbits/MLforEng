{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90369eff-3e22-4c64-9ddd-3389d8aed762",
   "metadata": {},
   "source": [
    "# 05 – Automated CommsCom Churn Training (Pipeline-Ready)\n",
    "\n",
    "In this module you will:\n",
    "\n",
    "1. Use **OpenShift AI Data Science Pipelines** to run an automated training job\n",
    "   for the CommsCom churn model.\n",
    "2. Upload the provided pipeline definition\n",
    "   `pipelines/commscom_churn_pipeline.yaml` into your OpenShift AI project.\n",
    "3. Create and run a pipeline **Run**.\n",
    "4. Inspect the **logs** and **output artifacts** (model + metadata) in the\n",
    "   Pipelines UI.\n",
    "\n",
    "> You do **not** need to build images or write YAML yourself in this module.\n",
    "> Everything is prebuilt; you will just use the OpenShift AI UI and this repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e83d88-1586-48ac-95c5-bd54949a919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd()\n",
    "while project_root.name != \"MLforEng\" and project_root != project_root.parent:\n",
    "    project_root = project_root.parent\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "pipeline_yaml = project_root / \"pipelines\" / \"commscom_churn_pipeline.yaml\"\n",
    "pipeline_yaml, pipeline_yaml.exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77b20e-3753-4371-a1dc-856f5c3f2aba",
   "metadata": {},
   "source": [
    "## Step 1 – Upload the pipeline YAML into OpenShift AI\n",
    "\n",
    "1. In a new browser tab, open the **OpenShift AI** web console.\n",
    "2. Go to **Data Science Projects** and select the project prepared for this lab\n",
    "   (for example: `commscom-ml`).\n",
    "3. In the left panel, click **Pipelines**.\n",
    "4. Click **Import pipeline** (or **Create pipeline** → **Upload a file**).\n",
    "5. In the dialog:\n",
    "   - Click **Browse** / **Choose file**.\n",
    "   - Navigate to your workbench home → `MLforEng/pipelines/`.\n",
    "   - Select `commscom_churn_pipeline.yaml`.\n",
    "6. Give the pipeline a name, for example:\n",
    "\n",
    "   - **Name**: `CommsCom Churn Training Pipeline`\n",
    "\n",
    "7. Click **Create** / **Import**.\n",
    "\n",
    "If the import is successful, you should see your pipeline listed with the name\n",
    "`commscom-churn-training-pipeline` and a small graph icon. When you click into\n",
    "it you will see **one step** called `train-commscom-churn` (or similar).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7c538-f09b-43f3-a4ff-9913016414b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2 – Create and run a Pipeline Run\n",
    "\n",
    "1. In the Pipelines page, click on your newly imported pipeline\n",
    "   (e.g. `CommsCom Churn Training Pipeline`).\n",
    "2. Click **Create run** (or **Start a new run**).\n",
    "3. In the run form:\n",
    "   - **Run name**: e.g. `commscom-churn-run-<your-initials>`.\n",
    "   - Under **Parameters**, leave the defaults:\n",
    "     - `model_family` = `rf`\n",
    "     - `test_size` = `0.2`\n",
    "4. Click **Start** / **Create run**.\n",
    "\n",
    "You will be taken to the **Run details** page.\n",
    "\n",
    "- You should see a DAG with a single node / step.\n",
    "- The status will transition: `Pending` → `Running` → `Succeeded`\n",
    "  (if everything is configured correctly).\n",
    "\n",
    "> If the run fails (e.g. ImagePull error), check with the instructor:\n",
    "> usually it means the training image is not accessible or the project does\n",
    "> not have a Pipelines server / storage connection configured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23c8c0-3829-4fb7-aa2a-1230cba55bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3 – Inspect the training logs\n",
    "\n",
    "1. In the **Run details** view, click on the step\n",
    "   (e.g. `train-commscom-churn`).\n",
    "2. Open the **Logs** tab.\n",
    "\n",
    "You should see output similar to what you saw when running training locally:\n",
    "\n",
    "- `=== CommsCom churn pipeline training step ===`\n",
    "- `Model family: rf`\n",
    "- A **classification report** for the test set.\n",
    "- `ROC–AUC: ...`\n",
    "- `Saved model to /tmp/output/model.joblib`\n",
    "- `Saved meta to /tmp/output/meta.json`\n",
    "- `=== Training step complete ===`\n",
    "\n",
    "This confirms that:\n",
    "\n",
    "- The pipeline successfully launched your\n",
    "  `quay.io/.../mlforeng-churn-train` image.\n",
    "- The training script `mlforeng.pipeline.train_churn_step` ran inside the\n",
    "  container.\n",
    "- The model artifacts were written to `/tmp/output` inside the container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45903a-8915-4387-91cf-c4e4dd4f00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4 – Inspect the output artifacts (model + metadata)\n",
    "\n",
    "1. Still on the step details view, switch to the **Artifacts** tab.\n",
    "2. You should see an artifact named something like `model_dir`.\n",
    "3. Click on `model_dir`.\n",
    "\n",
    "You will see the files that were written by the training container and copied\n",
    "into the pipeline output:\n",
    "\n",
    "- `model.joblib`\n",
    "- `meta.json`\n",
    "\n",
    "These files are actually stored in your project’s **object storage** (MinIO/S3)\n",
    "via the Data Science Pipelines server configuration. The Pipelines UI gives you\n",
    "a virtual view into that location.\n",
    "\n",
    "In a production setup, a second pipeline step or a separate CI/CD job might:\n",
    "\n",
    "- retrieve this artifact,\n",
    "- register it in a model registry,\n",
    "- or update a serving deployment configuration.\n",
    "\n",
    "For this workshop, the goal is to understand:\n",
    "\n",
    "- how custom training logic can be packaged as a container,\n",
    "- how it is orchestrated by OpenShift AI Pipelines,\n",
    "- and how model artifacts are captured and versioned centrally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ae084-5b57-4e23-b658-80971834432c",
   "metadata": {},
   "source": [
    "## Optional – Use the pipeline-trained model in local serving\n",
    "\n",
    "For the purposes of this workshop, the automated training pipeline and the\n",
    "local serving examples are **separate tracks**:\n",
    "\n",
    "- The pipeline writes `model.joblib` + `meta.json` to S3/MinIO.\n",
    "- The FastAPI serving examples expect models under\n",
    "  `artifacts/pretrained/<model_name>/` inside the repo.\n",
    "\n",
    "If you want to demonstrate using a pipeline-trained model with the local\n",
    "FastAPI server, you can:\n",
    "\n",
    "1. Download `model.joblib` + `meta.json` from the `model_dir` artifact in the\n",
    "   Pipelines UI.\n",
    "2. Place them under a new folder, for example:\n",
    "\n",
    "   ```text\n",
    "   artifacts/pretrained/commscom_rf_pipeline/\n",
    "     model.joblib\n",
    "     meta.json\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
