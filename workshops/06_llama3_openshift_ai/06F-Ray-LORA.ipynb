{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4303b0cf-e580-427a-ad02-f74c50d683cd",
   "metadata": {},
   "source": [
    "# 06E Module Works for Qwen and Llama (Hugging Face Login). This Notebook does LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc1183-031c-41f1-a7d2-325702e8a6cb",
   "metadata": {},
   "source": [
    "# Update your HF_TOKEN In notebook before running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a65a54d-41d9-4cef-8e5f-06eda7a9918a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess, json\n",
    "\n",
    "NS = \"ray-finetune-llm-deepspeed002\"\n",
    "\n",
    "def run(cmd):\n",
    "    return subprocess.check_output(cmd, text=True).strip()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ENVIRONMENT VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. OpenShift User:\")\n",
    "print(f\"   {run(['oc', 'whoami'])}\")\n",
    "\n",
    "print(\"\\n2. RayCluster Status:\")\n",
    "try:\n",
    "    data = json.loads(run([\"oc\", \"get\", \"raycluster\", \"ray\", \"-n\", NS, \"-o\", \"json\"]))\n",
    "    state = data.get(\"status\", {}).get(\"state\", \"unknown\")\n",
    "    workers = data.get(\"status\", {}).get(\"availableWorkerReplicas\", 0)\n",
    "    print(f\"   State: {state}\")\n",
    "    print(f\"   Workers: {workers}\")\n",
    "    \n",
    "    head_tols = data[\"spec\"][\"headGroupSpec\"][\"template\"][\"spec\"].get(\"tolerations\", [])\n",
    "    has_gpu = any(t.get(\"key\") == \"nvidia.com/gpu\" for t in head_tols)\n",
    "    print(f\"   GPU toleration: {has_gpu}\")\n",
    "    \n",
    "    if state != \"ready\":\n",
    "        print(\"   ‚ö†Ô∏è  Cluster not ready yet\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Environment check complete\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bb1ff-c51f-4f0c-8931-0b04042a2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from codeflare_sdk import TokenAuthentication\n",
    "\n",
    "token = subprocess.check_output([\"oc\", \"whoami\", \"-t\"]).decode().strip()\n",
    "server = subprocess.check_output([\"oc\", \"whoami\", \"--show-server=true\"]).decode().strip()\n",
    "\n",
    "auth = TokenAuthentication(\n",
    "    token=token,\n",
    "    server=server,\n",
    "    skip_tls=True\n",
    ")\n",
    "auth.login()\n",
    "\n",
    "print(f\"‚úÖ Authenticated to: {server}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e91d5-adab-4b72-becc-cd3748965378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May be no need to run. dont run.\n",
    "\n",
    "import os\n",
    "# p=\"/opt/app-root/src/models/facebook/opt-125m\"\n",
    "\n",
    "p=\"s3://ocpmodel\"\n",
    "print(\"dir exists:\", os.path.isdir(p))\n",
    "if os.path.isdir(p):\n",
    "    print(\"has config.json:\", os.path.exists(os.path.join(p,\"config.json\")))\n",
    "    print(\"files:\", sorted(os.listdir(p))[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f89474-d901-4d30-be0c-201604f080a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade --quiet --no-cache-dir \\\n",
    "    \"numpy==1.26.4\" \\\n",
    "    \"pyarrow==15.0.2\" \\\n",
    "    \"datasets==2.18.0\"\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22097bcb-e9e0-439d-8ee6-3d76c73fe36f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# WORKING_DIR = \"/opt/app-root/src/MLforEng\"\n",
    "\n",
    "# Option 1: Use local uploaded model (RECOMMENDED - no download needed)\n",
    "# LLM_MODEL_ID = \"/opt/app-root/src/models/llama-3.2-1b-instruct\"\n",
    "\n",
    "# LLM_MODEL_ID = \"/opt/app-root/src/models/facebook/opt-125m\"\n",
    "\n",
    "# LLM_MODEL_ID = \"facebook/opt-125m\"\n",
    "\n",
    "# LLM_MODEL_ID = \"gpt2\"\n",
    "\n",
    "\n",
    "# LLM_MODEL_ID = \"s3://ocpmodel/Qwen2.5-0.5B\"\n",
    "\n",
    "# Option 2: Use HuggingFace model (requires network + token)\n",
    "# LLM_MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# LLM_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# LLM_MODEL_ID = \"Qwen/Qwen2.5-0.5B\"\n",
    "LLM_MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "MODEL_BUCKET_URI = \"s3://ocpmodel\"\n",
    "\n",
    "# HuggingFace token (only needed for gated models like Llama)\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"YOURTOKEN\")\n",
    "\n",
    "# HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"hf_YOUR TOKEN\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET CONFIGURATION  \n",
    "# =============================================================================\n",
    "\n",
    "# Option 1: Use your custom JSONL dataset\n",
    "# DATASET_TYPE = \"jsonl\"\n",
    "# TRAIN_JSONL = \"artifacts/datasets/commscom_llama_prompts.jsonl\"  # Relative to working_dir\n",
    "# EVAL_JSONL = \"artifacts/datasets/commscom_llama_prompts.jsonl\"\n",
    "\n",
    "# Option 2: Use GSM8K demo dataset (math problems)\n",
    "DATASET_TYPE = \"gsm8k\"\n",
    "TRAIN_JSONL = \"\"  # Not used for gsm8k\n",
    "EVAL_JSONL = \"\"\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# OUTPUT_DIR = \"/opt/app-root/src/models/llama-finetuned\"\n",
    "# STORAGE_PATH = \"/opt/app-root/src\"\n",
    "OUTPUT_DIR = \"s3://ocpmodel/outputdir\"\n",
    "# STORAGE_PATH = \"s3://ocpmodel\"\n",
    "\n",
    "STORAGE_PATH = \"/opt/app-root/src/ray_results\"\n",
    "\n",
    "\n",
    "# Training hyperparameters\n",
    "MAX_STEPS = 30  # Small for demo; set to 0 to use NUM_TRAIN_EPOCHS\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "TRAIN_BATCH_SIZE = 2  # Can use 2 for 1B models, 1 for 8B\n",
    "EVAL_BATCH_SIZE = 2\n",
    "MAX_SEQ_LENGTH = 512\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_RATIO = 0.03\n",
    "SAVE_STEPS = 30\n",
    "EVAL_STEPS = 30\n",
    "SAVE_TOTAL_LIMIT = 2\n",
    "NUM_DEVICES = 7\n",
    "\n",
    "# Precision (use BF16 for modern GPUs like L4, A100)\n",
    "USE_BF16 = True\n",
    "USE_FP16 = False\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüì¶ Model:\")\n",
    "print(f\"   ID: {LLM_MODEL_ID}\")\n",
    "print(f\"   Type: {'Local' if LLM_MODEL_ID.startswith('s3') else 'HuggingFace'}\")\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"   Type: {DATASET_TYPE}\")\n",
    "if DATASET_TYPE == \"jsonl\":\n",
    "    print(f\"   Train: {TRAIN_JSONL}\")\n",
    "    print(f\"   Eval: {EVAL_JSONL}\")\n",
    "    \n",
    "    # Verify file exists\n",
    "    full_path = os.path.join(\"/opt/app-root/src/MLforEng\", TRAIN_JSONL)\n",
    "    if os.path.exists(full_path):\n",
    "        with open(full_path) as f:\n",
    "            num_lines = sum(1 for _ in f)\n",
    "        print(f\"   ‚úÖ File found: {num_lines} examples\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå File not found: {full_path}\")\n",
    "else:\n",
    "    print(f\"   Using GSM8K (will download automatically)\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Training:\")\n",
    "print(f\"   Max steps: {MAX_STEPS}\")\n",
    "print(f\"   Epochs: {NUM_TRAIN_EPOCHS}\")\n",
    "print(f\"   Batch size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"   Max sequence: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Precision: {'BF16' if USE_BF16 else 'FP16' if USE_FP16 else 'FP32'}\")\n",
    "\n",
    "print(f\"\\nüíæ Output:\")\n",
    "print(f\"   {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663844d-701a-46af-889d-65ac0391e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def diagnose_jsonl_file(file_path):\n",
    "    \"\"\"\n",
    "    Checks if a JSONL file exists, is readable, and contains valid JSON.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Diagnosing file: {file_path}\")\n",
    "    \n",
    "    # 1. Check existence and permissions\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå ERROR: File does not exist at the specified path.\")\n",
    "        return False\n",
    "    print(\"‚úÖ File exists.\")\n",
    "    \n",
    "    if not os.access(file_path, os.R_OK):\n",
    "        print(\"‚ùå ERROR: File exists but is not readable (check permissions).\")\n",
    "        return False\n",
    "    print(\"‚úÖ File is readable.\")\n",
    "    \n",
    "    # 2. Check file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"üìè File size: {file_size} bytes\")\n",
    "    if file_size == 0:\n",
    "        print(\"‚ùå ERROR: File is empty.\")\n",
    "        return False\n",
    "    \n",
    "    # 3. Validate JSONL format line by line\n",
    "    print(\"üß™ Validating JSONL format...\")\n",
    "    valid_lines = 0\n",
    "    total_lines = 0\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f, 1):\n",
    "                total_lines += 1\n",
    "                line = line.strip()\n",
    "                if not line:  # Skip empty lines\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    json.loads(line)\n",
    "                    valid_lines += 1\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"   ‚ùå Line {i}: JSON decode error - {e}\")\n",
    "                    print(f\"      Problematic line content: '{line[:100]}...'\")\n",
    "                    return False\n",
    "                    \n",
    "    except UnicodeDecodeError:\n",
    "        print(\"‚ùå ERROR: File is not UTF-8 encoded. Try different encoding.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: Could not read file - {e}\")\n",
    "        return False\n",
    "    \n",
    "    # 4. Summary\n",
    "    print(f\"üìä File Summary: {valid_lines} valid JSON objects out of {total_lines} lines.\")\n",
    "    \n",
    "    if valid_lines > 0:\n",
    "        print(\"‚úÖ File appears to be a valid JSONL.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå File contains no valid JSON objects.\")\n",
    "        return False\n",
    "\n",
    "# Run the diagnosis on your file\n",
    "file_path = \"/opt/app-root/src/MLforEng/artifacts/datasets/commscom_llama_prompts.jsonl\"\n",
    "is_valid = diagnose_jsonl_file(file_path)\n",
    "\n",
    "if not is_valid:\n",
    "    print(\"\\nüí° Next steps:\")\n",
    "    print(\"1. Check the specific line errors above\")\n",
    "    print(\"2. Open the file and verify it's pure JSONL (one JSON per line)\")\n",
    "    print(\"3. Ensure no BOM (Byte Order Mark) at file start\")\n",
    "    print(\"4. Try `encoding='utf-8-sig'` if you suspect BOM issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629e60a-90d5-4e9c-b134-f476cd2da73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "NS = \"ray-finetune-llm-deepspeed002\"\n",
    "ray_dashboard_url = f\"http://ray-head-svc.{NS}.svc.cluster.local:8265\"\n",
    "\n",
    "client = JobSubmissionClient(ray_dashboard_url)\n",
    "\n",
    "# Verify connection\n",
    "jobs = client.list_jobs()\n",
    "print(f\"‚úÖ Connected to Ray: {ray_dashboard_url}\")\n",
    "print(f\"   Existing jobs: {len(jobs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1a8d1-4eb9-4f4b-8f44-f06909ef173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "MLFORENG_ROOT = \"/opt/app-root/src/MLforEng\"\n",
    "# Verify DeepSpeed config exists\n",
    "# ds_config_path = \"./deepspeed_configs/zero_3_offload_optim_param.json\"\n",
    "# ds_config_path = \"/opt/app-root/src/MLforEng/mlforeng/llm_finetune/deepspeed_configs/zero_3_offload_optim_param.json\"\n",
    "\n",
    "ds_config_path = f\"{MLFORENG_ROOT}/mlforeng/llm_finetune/deepspeed_configs/zero_3_offload_optim_param.json\"\n",
    "\n",
    "if os.path.exists(ds_config_path):\n",
    "    with open(ds_config_path, 'r') as f:\n",
    "        ds_config = json.load(f)\n",
    "\n",
    "    print(f\"‚úÖ DeepSpeed config found: {ds_config_path}\")\n",
    "    print(f\"   ZeRO stage: {ds_config.get('zero_optimization', {}).get('stage', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"‚ùå DeepSpeed config NOT found: {ds_config_path}\")\n",
    "    print(\"\\\\nCreating default DeepSpeed ZeRO-3 config...\")\n",
    "    \n",
    "    os.makedirs(\"./deepspeed_configs\", exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    default_config = {\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"gradient_clipping\": 1.0,\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"sub_group_size\": 1e9,\n",
    "            \"reduce_bucket_size\": 5e8,\n",
    "            \"stage3_prefetch_bucket_size\": 5e8,\n",
    "            \"stage3_param_persistence_threshold\": 1e6,\n",
    "            \"stage3_max_live_parameters\": 1e9,\n",
    "            \"stage3_max_reuse_distance\": 1e9,\n",
    "            \"stage3_gather_16bit_weights_on_model_save\": True\n",
    "        },\n",
    "        \"fp16\": {\n",
    "            \"enabled\": True,\n",
    "            \"loss_scale\": 0,\n",
    "            \"loss_scale_window\": 1000,\n",
    "            \"initial_scale_power\": 16,\n",
    "            \"hysteresis\": 2,\n",
    "            \"min_loss_scale\": 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(ds_config_path, 'w') as f:\n",
    "        json.dump(default_config, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Created: {ds_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d35a012-3c08-4850-9316-2644f2126e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base64 encoding:\n",
    "import base64\n",
    "import json\n",
    "\n",
    "# Read the DeepSpeed config file and convert to base64\n",
    "ds_config_path = \"/opt/app-root/src/MLforEng/mlforeng/llm_finetune/deepspeed_configs/zero_3_offload_optim_param.json\"\n",
    "try:\n",
    "    with open(ds_config_path, 'r') as f:\n",
    "        ds_config_content = f.read()\n",
    "    ds_config_b64 = base64.b64encode(ds_config_content.encode()).decode()\n",
    "    print(f\"‚úÖ DeepSpeed config encoded to base64 ({len(ds_config_b64)} chars)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to read config file: {e}\")\n",
    "    # Fall back to default config as base64\n",
    "    default_config = {\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "            \"offload_param\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True\n",
    "        },\n",
    "        \"fp16\": {\"enabled\": True}\n",
    "    }\n",
    "    config_json = json.dumps(default_config)\n",
    "    ds_config_b64 = base64.b64encode(config_json.encode()).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f06e6-eb8f-4691-8b5c-12d12e661e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For S3: Set Ray storage path to the bucket; this can also be passed as runtime_env var also\n",
    "os.environ[\"RAY_AIR_DEFAULT_STORAGE\"] = \"s3://ocpmodel/ray-results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c90a5-601e-4c83-8e2d-1d509c1320a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW VERSION TO SUBMIT JOB FROM DS MODEL\n",
    "print(\"Submitting training job...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build runtime environment\n",
    "runtime_env = {\n",
    "    # 1. FIXED: Use the correct module path for packaging\n",
    "    \"py_modules\": [\"/opt/app-root/src/MLforEng/mlforeng\"],\n",
    "    \"working_dir\": \"/opt/app-root/src/MLforEng\",\n",
    "    # \"torch>=2.6.0\",\n",
    "    \"pip\": [\n",
    "      # Use pinned versions for stability during debugging\n",
    "        \"deepspeed>=0.9.0\",\n",
    "        \"bitsandbytes>=0.39.0\",\n",
    "        \"scipy\",\n",
    "        \"scikit-learn\",\n",
    "        \"accelerate==0.31.0\",\n",
    "        \"awscliv2==2.3.0\",\n",
    "        \"datasets==2.19.0\",\n",
    "        \"peft==0.11.1\",\n",
    "        \"transformers==4.44.0\",\n",
    "        \"sentencepiece\",\n",
    "     ],\n",
    "    \"env_vars\": {\n",
    "        # 2. FIXED: Consolidated config into ENVIRONMENT VARIABLES only\n",
    "       # \"_FORCE_ENV_REBUILD\": str(int(time.time())),  # Force fresh environment\n",
    "        \n",
    "        # Model Config (Your script reads these)\n",
    "        \"LLM_MODEL_ID\": LLM_MODEL_ID,  # e.g., \"Qwen/Qwen2.5-0.5B\"\n",
    "        \"HF_TOKEN\": HF_TOKEN,\n",
    "        \"MODEL_BUCKET_URI\": MODEL_BUCKET_URI,\n",
    "        # Dataset Config\n",
    "        \"DATASET_TYPE\": DATASET_TYPE,   # e.g., \"gsm8k\" or \"jsonl\"\n",
    "        \"TRAIN_JSONL\": TRAIN_JSONL if DATASET_TYPE == \"jsonl\" else \"\",\n",
    "        \"EVAL_JSONL\": EVAL_JSONL if DATASET_TYPE == \"jsonl\" else \"\",\n",
    "\n",
    "        \"AWS_ACCESS_KEY_ID\": \"<ACCESSKEY>\",\n",
    "        \"AWS_SECRET_ACCESS_KEY\": \"<SECRET KEY>\",\n",
    "        \"AWS_DEFAULT_REGION\": \"us-east-2\",\n",
    "\n",
    "        # Training Config\n",
    "        \"OUTPUT_DIR\": OUTPUT_DIR,\n",
    "        \"MAX_STEPS\": \"30\",\n",
    "        \"NUM_TRAIN_EPOCHS\": str(NUM_TRAIN_EPOCHS),\n",
    "        \"TRAIN_BATCH_SIZE\": str(TRAIN_BATCH_SIZE),\n",
    "        \"EVAL_BATCH_SIZE\": str(EVAL_BATCH_SIZE),\n",
    "        \"USE_BF16\": \"1\",\n",
    "        \n",
    "        # DeepSpeed Config - Pass config name as an env var\n",
    "        # \"DEEPSPEED_CONFIG\": \"zero_3_offload_optim_param.json\",\n",
    "        \"DEEPSPEED_CONFIG\": ds_config_b64,\n",
    "\n",
    "        \"DEEPSPEED_CONFIG_BASE64\": ds_config_b64,\n",
    "        \n",
    "        # Cache Paths\n",
    "        \"HF_HOME\": f\"{STORAGE_PATH}/.cache\",\n",
    "        \"TRANSFORMERS_CACHE\": f\"{STORAGE_PATH}/.cache/transformers\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# 3. FIXED: Simple entrypoint with NO command-line arguments.\n",
    "# Your script should read all config from the environment variables above.\n",
    "entrypoint = \"python -m mlforeng.llm_finetune.ray_finetune_llm_deepspeed --lora\"\n",
    "\n",
    "# entrypoint = (\n",
    "#    \"python -m mlforeng.llm_finetune.ray_finetune_llm_deepspeed\"\n",
    "#    f\"--model-name {LLM_MODEL_ID} \"\n",
    "#    f\"--train-path {TRAIN_JSONL} \"\n",
    "#    f\"--test-path {EVAL_JSONL} \"\n",
    "#    f\"--dataset-config artifacts/datasets/config.json \"  # or your cfg\n",
    "#    f\"--output-dir {OUTPUT_DIR} \"\n",
    "#    f\"--ds-config /opt/app-root/src/MLforEng/deepspeed_configs/zero_3_offload_optim_param.json \"\n",
    "#    f\"--storage-path {STORAGE_PATH}\"\n",
    "# )\n",
    "\n",
    "# entrypoint = \"python -m ray_finetune_llm_deepspeed\"\n",
    "\n",
    "try:\n",
    "    submission_id = client.submit_job(\n",
    "        entrypoint=entrypoint,  # This is now a simple string\n",
    "        runtime_env=runtime_env\n",
    "    )\n",
    "    print(f\"‚úÖ Job submitted successfully!\")\n",
    "    print(f\"\\nüìã Job ID: {submission_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Submission failed: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# --- DEBUG: show what this notebook THINKS it's using ---\n",
    "\n",
    "import sys, os, importlib, subprocess, textwrap\n",
    "\n",
    "print(\"\\n=== RAY JOB RUNTIME_ENV (client-side view) ===\")\n",
    "try:\n",
    "    print(\"RAY jobs client address:\", getattr(client, \"address\", \"<not available>\"))\n",
    "except NameError:\n",
    "    print(\"RAY jobs client is not defined in this notebook cell.\")\n",
    "\n",
    "print(\"RAY_ADDRESS used for submission:\", ray_dashboard_url)\n",
    "print(\"working_dir:\", runtime_env.get(\"working_dir\"))\n",
    "print(\"py_modules:\", runtime_env.get(\"py_modules\"))\n",
    "print(\"pip packages passed into runtime_env:\")\n",
    "for p in runtime_env.get(\"pip\", []):\n",
    "    print(\"  -\", p)\n",
    "\n",
    "print(\"\\nenv_vars passed into runtime_env:\")\n",
    "print(json.dumps(runtime_env.get(\"env_vars\", {}), indent=2))\n",
    "\n",
    "print(\"\\n=== LOCAL NOTEBOOK PYTHON & PATHS (not inside Ray job) ===\")\n",
    "print(\"Notebook sys.executable:\", sys.executable)\n",
    "print(\"Notebook CWD:\", os.getcwd())\n",
    "\n",
    "# Where is the 'ray' CLI coming from?\n",
    "try:\n",
    "    ray_path = subprocess.run(\n",
    "        [\"which\", \"ray\"], capture_output=True, text=True, check=False\n",
    "    ).stdout.strip()\n",
    "    print(\"`ray` CLI path:\", ray_path or \"<which ray returned nothing>\")\n",
    "except Exception as e:\n",
    "    print(\"Could not run `which ray`:\", e)\n",
    "\n",
    "# Where is the training module located (as seen from notebook)?\n",
    "try:\n",
    "    import mlforeng.llm_finetune.ray_finetune_llm_deepspeed as rfl\n",
    "\n",
    "    print(\"Training module file:\", os.path.abspath(rfl.__file__))\n",
    "except Exception as e:\n",
    "    print(\"Could not import mlforeng.llm_finetune.ray_finetune_llm_deepspeed:\", e)\n",
    "\n",
    "print(\"\\n=== KEY LIBRARY VERSIONS IN NOTEBOOK ENV ===\")\n",
    "for name in [\"torch\", \"transformers\", \"deepspeed\", \"accelerate\", \"datasets\", \"peft\"]:\n",
    "    try:\n",
    "        m = importlib.import_module(name)\n",
    "        version = getattr(m, \"__version__\", \"unknown\")\n",
    "        mod_file = getattr(m, \"__file__\", \"<no __file__>\")\n",
    "        print(f\"  {name}: {version} ({mod_file})\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {name}: NOT INSTALLED ({e})\")\n",
    "\n",
    "# requirements.txt sanity check (as seen from notebook)\n",
    "req_path = os.path.join(\"/opt/app-root/src/MLforEng\", \"requirements.txt\")\n",
    "print(\"\\nrequirements.txt exists:\", os.path.exists(req_path), \"->\", req_path)\n",
    "if os.path.exists(req_path):\n",
    "    print(\"First 20 lines of requirements.txt:\")\n",
    "    try:\n",
    "        with open(req_path) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 20:\n",
    "                    break\n",
    "                print(\"   \", line.rstrip())\n",
    "    except Exception as e:\n",
    "        print(\"Could not read requirements.txt:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a28dd-b00f-463e-8ad1-922ad1db0d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
