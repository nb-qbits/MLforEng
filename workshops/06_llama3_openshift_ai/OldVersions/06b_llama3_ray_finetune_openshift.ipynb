{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abc995-78c7-4b6b-a34f-af95e2335ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working\n",
    "import subprocess\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Configuration\n",
    "NS = \"ray-finetune-llm-deepspeed002\"\n",
    "CLUSTER_NAME = \"ray\"\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    \"\"\"Run shell command and return output.\"\"\"\n",
    "    return subprocess.check_output(cmd, text=True).strip()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ENVIRONMENT VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check authentication\n",
    "print(\"\\n1. OpenShift Authentication:\")\n",
    "whoami = run_cmd([\"oc\", \"whoami\"])\n",
    "print(f\"   User: {whoami}\")\n",
    "\n",
    "# Check Ray cluster exists\n",
    "print(\"\\n2. Ray Cluster Status:\")\n",
    "try:\n",
    "    rc_json = run_cmd([\"oc\", \"get\", \"raycluster\", CLUSTER_NAME, \"-n\", NS, \"-o\", \"json\"])\n",
    "    rc_data = json.loads(rc_json)\n",
    "    \n",
    "    state = rc_data.get(\"status\", {}).get(\"state\", \"unknown\")\n",
    "    ready_workers = rc_data.get(\"status\", {}).get(\"availableWorkerReplicas\", 0)\n",
    "    desired_workers = rc_data.get(\"status\", {}).get(\"desiredWorkerReplicas\", 0)\n",
    "    \n",
    "    print(f\"   Cluster: {CLUSTER_NAME}\")\n",
    "    print(f\"   State: {state}\")\n",
    "    print(f\"   Workers: {ready_workers}/{desired_workers}\")\n",
    "    \n",
    "    if state != \"ready\":\n",
    "        print(\"   ‚ö†Ô∏è  WARNING: Cluster not ready yet. Wait a few minutes.\")\n",
    "    \n",
    "    # Check GPU tolerations\n",
    "    head_tolerations = rc_data[\"spec\"][\"headGroupSpec\"][\"template\"][\"spec\"].get(\"tolerations\", [])\n",
    "    has_gpu_tol = any(\n",
    "        t.get(\"key\") == \"nvidia.com/gpu\" and t.get(\"effect\") == \"NoSchedule\"\n",
    "        for t in head_tolerations\n",
    "    )\n",
    "    print(f\"   GPU Toleration: {has_gpu_tol}\")\n",
    "    \n",
    "except subprocess.CalledProcessError:\n",
    "    print(f\"   ‚ùå ERROR: RayCluster '{CLUSTER_NAME}' not found!\")\n",
    "    print(f\"   ‚Üí Apply: oc apply -f 02_ray_localqueue_and_cluster.yaml\")\n",
    "    raise\n",
    "\n",
    "# Check Ray pods\n",
    "print(\"\\n3. Ray Pods:\")\n",
    "try:\n",
    "    pods_output = run_cmd([\"oc\", \"get\", \"pods\", \"-n\", NS, \"-l\", \"ray.io/cluster=ray\", \"--no-headers\"])\n",
    "    if pods_output:\n",
    "        pods = pods_output.split(\"\\n\")\n",
    "        print(f\"   Found {len(pods)} pod(s):\")\n",
    "        for pod in pods:\n",
    "            parts = pod.split()\n",
    "            if len(parts) >= 3:\n",
    "                print(f\"     - {parts[0]}: {parts[2]}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No Ray pods found (cluster may be suspended)\")\n",
    "except:\n",
    "    print(\"   ‚ö†Ô∏è  Could not retrieve pod status\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Prerequisites verified!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6775bf-d922-4a13-a32c-eb66c3df2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages needed for this notebook\n",
    "!{sys.executable} -m pip install --quiet --upgrade \\\n",
    "    \"numpy==1.26.4\" \\\n",
    "    \"pyarrow==15.0.2\" \\\n",
    "    \"datasets==2.18.0\"\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8fdd33-35aa-464d-ab6b-4c51e525fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeflare_sdk import TokenAuthentication\n",
    "\n",
    "# Get token and server from oc CLI\n",
    "token = subprocess.check_output([\"oc\", \"whoami\", \"-t\"]).decode().strip()\n",
    "server = subprocess.check_output([\"oc\", \"whoami\", \"--show-server=true\"]).decode().strip()\n",
    "\n",
    "# Authenticate\n",
    "auth = TokenAuthentication(\n",
    "    token=token,\n",
    "    server=server,\n",
    "    skip_tls=True  # Set False if your cluster has proper TLS certs\n",
    ")\n",
    "auth.login()\n",
    "\n",
    "print(f\"‚úÖ Authenticated to: {server}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279b9c9-ceec-4981-8d4b-e1cff3969b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.job_submission import JobSubmissionClient\n",
    "import subprocess\n",
    "import json, os\n",
    "\n",
    "NS = \"ray-finetune-llm-deepspeed002\"\n",
    "\n",
    "print(\"Connecting to Ray cluster...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Auto-discover dashboard URL\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"oc\", \"get\", \"raycluster\", \"ray\", \"-n\", NS, \"-o\", \"jsonpath={.status.head.serviceName}\"],\n",
    "        capture_output=True, text=True, check=True\n",
    "    )\n",
    "    head_service = result.stdout.strip()\n",
    "    \n",
    "    if head_service:\n",
    "        ray_dashboard_url = f\"http://{head_service}.{NS}.svc.cluster.local:8265\"\n",
    "        print(f\"‚úÖ Auto-discovered: {ray_dashboard_url}\")\n",
    "    else:\n",
    "        raise Exception(\"No service name found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Auto-discovery failed, trying common names...\")\n",
    "    \n",
    "    # Try common service names\n",
    "    for svc_name in [\"ray-head-svc\", \"ray-cluster-head-svc\", \"raycluster-head-svc\"]:\n",
    "        ray_dashboard_url = f\"http://{svc_name}.{NS}.svc.cluster.local:8265\"\n",
    "        try:\n",
    "            test_client = JobSubmissionClient(ray_dashboard_url)\n",
    "            test_client.list_jobs()\n",
    "            print(f\"‚úÖ Found working URL: {ray_dashboard_url}\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        raise RuntimeError(\"Could not connect to Ray dashboard!\")\n",
    "\n",
    "# Connect\n",
    "client = JobSubmissionClient(ray_dashboard_url)\n",
    "\n",
    "# Verify\n",
    "jobs = client.list_jobs()\n",
    "print(f\"‚úÖ Connected successfully!\")\n",
    "print(f\"   URL: {ray_dashboard_url}\")\n",
    "print(f\"   Existing jobs: {len(jobs)}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30c26b-d439-4d74-b3fe-d9e84db29a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create requirements.txt for Ray runtime environment\n",
    "requirements_content = \"\"\"torch>=2.0.0\n",
    "transformers>=4.30.0\n",
    "datasets>=2.18.0\n",
    "accelerate>=0.20.0\n",
    "deepspeed>=0.9.0\n",
    "peft>=0.4.0\n",
    "bitsandbytes>=0.39.0\n",
    "scipy\n",
    "\"\"\"\n",
    "\n",
    "# Write requirements.txt\n",
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(\"‚úÖ Created requirements.txt:\")\n",
    "print(requirements_content)\n",
    "# Know your Current Working Directory for any references\n",
    "print(f\"\\nüìç Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237ad17-59ef-4b5b-8238-f2204b616d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MLFORENG_ROOT = \"/opt/app-root/src/MLforEng\"\n",
    "\n",
    "# Create setup.py\n",
    "setup_content = '''from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"mlforeng\",\n",
    "    version=\"0.1.0\",\n",
    "    packages=find_packages(),\n",
    "    install_requires=[\n",
    "        \"torch>=2.0.0\",\n",
    "        \"transformers>=4.30.0\",\n",
    "        \"datasets>=2.18.0\",\n",
    "        \"accelerate>=0.20.0\",\n",
    "        \"deepspeed>=0.9.0\",\n",
    "        \"peft>=0.4.0\",\n",
    "        \"bitsandbytes>=0.39.0\",\n",
    "        \"scipy\",\n",
    "    ],\n",
    "    python_requires=\">=3.9\",\n",
    ")\n",
    "'''\n",
    "\n",
    "setup_path = os.path.join(MLFORENG_ROOT, \"setup.py\")\n",
    "\n",
    "with open(setup_path, \"w\") as f:\n",
    "    f.write(setup_content)\n",
    "\n",
    "print(f\"‚úÖ Created: {setup_path}\")\n",
    "print(\"\\nContents:\")\n",
    "print(setup_content)\n",
    "print(\"\\n‚úÖ Now you can submit the job with:\")\n",
    "print('   \"pip\": [\"requirements.txt\", \".\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74c768-1817-4efb-bbe3-aa258503b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "MLFORENG_ROOT = \"/opt/app-root/src/MLforEng\"\n",
    "STORAGE_PATH = \"/opt/app-root/src\"\n",
    "\n",
    "# Read DeepSpeed config as dictionary\n",
    "\n",
    "ds_config_path = f\"{MLFORENG_ROOT}/mlforeng/llm_finetune/deepspeed_configs/zero_3_offload_optim_param.json\"\n",
    "\n",
    "if os.path.exists(ds_config_path):\n",
    "    with open(ds_config_path, 'r') as f:\n",
    "        ds_config_dict = json.load(f)\n",
    "    print(\"‚úÖ DeepSpeed config loaded as dictionary\")\n",
    "else:\n",
    "    print(f\"‚ùå DeepSpeed config not found: {ds_config_path}\")\n",
    "    ds_config_dict = None\n",
    "\n",
    "# Convert to base64 encoded string for Ray\n",
    "import base64\n",
    "ds_config_base64 = base64.b64encode(json.dumps(ds_config_dict).encode()).decode()\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"model_name\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "    \"use_lora\": True,\n",
    "    \"num_devices\": 2,\n",
    "    \"num_epochs\": 1,\n",
    "    \"batch_size_per_device\": 1,\n",
    "    \"eval_batch_size_per_device\": 1,\n",
    "    \"storage_path\": f\"{STORAGE_PATH}/ray_finetune_output/\",\n",
    "    \"ds_config\": ds_config_base64,  # ‚Üê Base64 encoded string\n",
    "    \"ctx_len\": 256,\n",
    "    \"lora_config\": \"mlforeng/llm_finetune/lora_configs/lora.json\"\n",
    "}\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    if key == \"ds_config\":\n",
    "        print(f\"  {key}: <base64 encoded> ({len(value)} chars)\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777186a-7b44-45fe-af5b-15afdea8ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting training job to Ray cluster...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build entrypoint command\n",
    "# CRITICAL: Use -m flag because script has relative imports\n",
    "entrypoint_cmd = (\n",
    "    f\"python -m mlforeng.llm_finetune.ray_finetune_llm_deepspeed \"\n",
    "    f\"--model-name={TRAINING_CONFIG['model_name']} \"\n",
    "    f\"{'--lora ' if TRAINING_CONFIG['use_lora'] else ''}\"\n",
    "    f\"--lora-config=mlforeng/llm_finetune/lora_configs/lora.json \"\n",
    "    f\"--num-devices={TRAINING_CONFIG['num_devices']} \"\n",
    "    f\"--num-epochs={TRAINING_CONFIG['num_epochs']} \"\n",
    "#    f\"--max-steps={TRAINING_CONFIG['max_steps']} \"\n",
    "    f\"--ds-config={TRAINING_CONFIG['ds_config']} \"\n",
    "    f\"--storage-path={TRAINING_CONFIG['storage_path']} \"\n",
    "    f\"--batch-size-per-device={TRAINING_CONFIG['batch_size_per_device']} \"\n",
    "    f\"--eval-batch-size-per-device={TRAINING_CONFIG['eval_batch_size_per_device']} \"\n",
    "    f\"--ctx-len={TRAINING_CONFIG['ctx_len']} \"  # Memory optimization\n",
    "    f\"--as-test\" # Quick Test Mode\n",
    ")\n",
    "\n",
    "print(\"Entrypoint command:\")\n",
    "print(f\"  {entrypoint_cmd}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Runtime environment configuration\n",
    "runtime_env = {\n",
    "    # Working directory: MLforEng root\n",
    "    \"working_dir\": MLFORENG_ROOT,\n",
    "    \"pip\": [\n",
    "        # Core ML/DL packages\n",
    "        \"torch>=2.0.0\",\n",
    "        \"transformers>=4.30.0\",\n",
    "        \"datasets>=2.18.0\",\n",
    "        \"accelerate>=0.20.0\",\n",
    "        \"deepspeed>=0.9.0\",\n",
    "        \"peft>=0.4.0\",\n",
    "        \"bitsandbytes>=0.39.0\",\n",
    "        \n",
    "        # Data science packages\n",
    "        \"scikit-learn\",\n",
    "        \"scipy\",\n",
    "        \"pandas\",\n",
    "        \"numpy\",\n",
    "        \"joblib\",\n",
    "        \"awscliv2\",\n",
    "        \"boto3\",\n",
    "        \n",
    "        \n",
    "        # Other utilities\n",
    "        \"tqdm\",\n",
    "        \"sentencepiece\",\n",
    "        \"protobuf\",\n",
    "    ],\n",
    "    \n",
    "    # Environment variables\n",
    "    \"env_vars\": {\n",
    "        # Add MLforEng to Python path\n",
    "        \"PYTHONPATH\": MLFORENG_ROOT,\n",
    "        \n",
    "        # HuggingFace cache location\n",
    "        \"HF_HOME\": f\"{STORAGE_PATH}/.cache\",\n",
    "        \"TRANSFORMERS_CACHE\": f\"{STORAGE_PATH}/.cache/transformers\",\n",
    "    },\n",
    "    \n",
    "    # Exclude unnecessary files\n",
    "    \"excludes\": [\n",
    "        \"/docs/\", \n",
    "        \"*.ipynb\", \n",
    "        \"*.md\", \n",
    "        \".git/\", \n",
    "        \"__pycache__/\",\n",
    "        \"/workshops/\",\n",
    "        \"*.pyc\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Runtime environment:\")\n",
    "print(f\"  Working dir: {runtime_env['working_dir']}\")\n",
    "print(f\"  Pip install: {runtime_env['pip']}\")\n",
    "print(f\"  PYTHONPATH: {runtime_env['env_vars']['PYTHONPATH']}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Submit the job\n",
    "    submission_id = client.submit_job(\n",
    "        entrypoint=entrypoint_cmd,\n",
    "        runtime_env=runtime_env\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Job submitted successfully!\")\n",
    "    print(f\"\\nüìã Job ID: {submission_id}\")\n",
    "    print(f\"\\n‚è±Ô∏è  Initial status: {client.get_job_status(submission_id)}\")\n",
    "    print(f\"\\nüí° Monitor progress in the next cell\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Job submission failed!\")\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(f\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Check MLforEng has setup.py or pyproject.toml\")\n",
    "    print(\"   2. Verify all paths in config cell show ‚úÖ\")\n",
    "    print(\"   3. Check Ray cluster is running\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb461859-23df-4251-bbdc-6200f47aaf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"Monitoring training job...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Job ID: {submission_id}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prev_log_length = 0\n",
    "check_interval = 30  # Check every 30 seconds\n",
    "max_checks = 120  # Max 60 minutes (120 * 30 seconds)\n",
    "\n",
    "for iteration in range(max_checks):\n",
    "    try:\n",
    "        # Get current status\n",
    "        status = client.get_job_status(submission_id)\n",
    "        \n",
    "        # Get logs\n",
    "        logs = client.get_job_logs(submission_id)\n",
    "        lines = logs.split('\\n') if logs else []\n",
    "        \n",
    "        # Show new log lines\n",
    "        if len(lines) > prev_log_length:\n",
    "            new_lines = lines[prev_log_length:]\n",
    "            \n",
    "            # Filter for important lines\n",
    "            important_keywords = [\n",
    "                'step', 'epoch', 'loss', 'loading', 'downloading',\n",
    "                'error', 'training', 'started', 'completed', 'saving'\n",
    "            ]\n",
    "            \n",
    "            for line in new_lines:\n",
    "                line_lower = line.lower()\n",
    "                if any(keyword in line_lower for keyword in important_keywords):\n",
    "                    print(line)\n",
    "            \n",
    "            prev_log_length = len(lines)\n",
    "        \n",
    "        # Check if job finished\n",
    "        if status in [\"SUCCEEDED\", \"FAILED\", \"STOPPED\"]:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(f\"‚úÖ Job finished with status: {status}\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            if status == \"SUCCEEDED\":\n",
    "                print(\"\\nüéâ Training completed successfully!\")\n",
    "                print(f\"\\nüìÅ Model saved to: {TRAINING_CONFIG['storage_path']}\")\n",
    "                print(\"\\n‚úÖ Proceed to next cell to check results\")\n",
    "            elif status == \"FAILED\":\n",
    "                print(\"\\n‚ùå Training failed!\")\n",
    "                print(\"\\nüìã Check the full logs above for error details\")\n",
    "                print(\"\\nüí° Common issues:\")\n",
    "                print(\"   - Out of GPU memory (reduce batch size)\")\n",
    "                print(\"   - Model download failed (check network/auth)\")\n",
    "                print(\"   - Missing dependencies (check requirements.txt)\")\n",
    "            break\n",
    "        \n",
    "        # Show periodic status update\n",
    "        if iteration % 10 == 0:  # Every 5 minutes\n",
    "            elapsed = iteration * check_interval\n",
    "            print(f\"\\n[{elapsed}s elapsed] Status: {status} - Still monitoring...\")\n",
    "        \n",
    "        time.sleep(check_interval)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è  Monitoring interrupted by user\")\n",
    "        print(f\"\\nJob is still running. Current status: {client.get_job_status(submission_id)}\")\n",
    "        print(f\"\\nTo resume monitoring, re-run this cell\")\n",
    "        print(f\"To stop the job, run: client.stop_job('{submission_id}')\")\n",
    "        break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error monitoring job: {e}\")\n",
    "        break\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Monitoring timeout reached\")\n",
    "    print(f\"Job status: {client.get_job_status(submission_id)}\")\n",
    "    print(f\"\\nThe job may still be running. Check Ray dashboard or re-run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d340f86-1a04-48d3-a5e7-067faecfc17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Checking training results...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get final job status\n",
    "final_status = client.get_job_status(submission_id)\n",
    "print(f\"Final job status: {final_status}\\n\")\n",
    "\n",
    "# Check if output directory exists\n",
    "output_dir = TRAINING_CONFIG['storage_path']\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"‚úÖ Output directory exists: {output_dir}\\n\")\n",
    "    \n",
    "    # List files in output directory\n",
    "    print(\"Files in output directory:\")\n",
    "    try:\n",
    "        for root, dirs, files in os.walk(output_dir):\n",
    "            level = root.replace(output_dir, '').count(os.sep)\n",
    "            indent = ' ' * 2 * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            subindent = ' ' * 2 * (level + 1)\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                size = os.path.getsize(file_path)\n",
    "                size_mb = size / (1024 * 1024)\n",
    "                print(f\"{subindent}{file} ({size_mb:.2f} MB)\")\n",
    "                if level > 2:  # Limit depth\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"  Error listing files: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå Output directory not found: {output_dir}\")\n",
    "    print(\"\\nThis could mean:\")\n",
    "    print(\"  - Training hasn't completed yet\")\n",
    "    print(\"  - Training failed before saving\")\n",
    "    print(\"  - Storage path is incorrect\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Show full logs if job failed\n",
    "if final_status == \"FAILED\":\n",
    "    print(\"\\nüìã Full job logs:\")\n",
    "    print(\"=\" * 70)\n",
    "    full_logs = client.get_job_logs(submission_id)\n",
    "    print(full_logs[-5000:])  # Last 5000 characters\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee11013-8646-4cda-94a2-f8e731baa1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to stop the job\n",
    "# client.stop_job(submission_id)\n",
    "# print(f\"‚úÖ Job {submission_id} stopped\")\n",
    "\n",
    "print(\"üí° Uncomment the lines above to stop the job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca87c52-2dc7-4635-bd66-4bf2349fb8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, json\n",
    "\n",
    "NAMESPACE = \"ray-finetune-llm-deepspeed002\"\n",
    "RAY_CLUSTER_NAME = \"ray\"   # what you used in the RayCluster yaml\n",
    "\n",
    "def get_ray_dashboard_url(namespace: str = NAMESPACE, cluster_name: str = RAY_CLUSTER_NAME) -> str:\n",
    "    \"\"\"\n",
    "    Find the Route created for the Ray dashboard and return a HTTPS URL.\n",
    "    In RHOAI, the route name convention is 'rayclient-<cluster-name>'.\n",
    "    \"\"\"\n",
    "    route_name = f\"ray-dashboard-{cluster_name}\"\n",
    "    \n",
    "    raw = subprocess.check_output(\n",
    "        [\n",
    "            \"oc\",\n",
    "            \"get\",\n",
    "            \"route\",\n",
    "            route_name,\n",
    "            \"-n\",\n",
    "            namespace,\n",
    "            \"-o\",\n",
    "            \"json\",\n",
    "        ]\n",
    "    ).decode()\n",
    "    data = json.loads(raw)\n",
    "    host = data[\"spec\"][\"host\"]\n",
    "    # Ray dashboard default root, you can jump straight to Jobs tab with /#/jobs\n",
    "    return f\"https://{host}/#/overview\"\n",
    "\n",
    "dash_url = get_ray_dashboard_url()\n",
    "print(\"‚úÖ Ray dashboard URL:\")\n",
    "print(dash_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feba9dc-e36b-4490-a311-408daf833597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 06b: Llama 3 Fine-Tuning with Ray on OpenShift AI\n",
    "\n",
    "## üéØ **Overview**\n",
    "\n",
    "This notebook submits a distributed training job to an existing Ray cluster for fine-tuning Llama 3.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Ray cluster deployed (via `02_ray_localqueue_and_cluster.yaml`)\n",
    "- Kueue configured (via `01_gpu_flavor_and_queue.yaml`)\n",
    "- RBAC permissions (via `03_rbac_notebook_ray.yaml`)\n",
    "- Training code and data uploaded to `/opt/app-root/src`\n",
    "\n",
    "**Expected Time:** 1.5-2 hours for full training\n",
    "\n",
    "**Steps:**\n",
    "1. Verify prerequisites\n",
    "2. Configure training parameters\n",
    "3. Submit training job to Ray\n",
    "4. Monitor training progress\n",
    "5. Retrieve results\n",
    "\n",
    "## 1Ô∏è‚É£ Verify Environment & Prerequisites\n",
    "\n",
    "Check that we're authenticated and the Ray cluster exists.\n",
    "import subprocess\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Configuration\n",
    "NS = \"ray-finetune-llm-deepspeed002\"\n",
    "CLUSTER_NAME = \"ray\"\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    \"\"\"Run shell command and return output.\"\"\"\n",
    "    return subprocess.check_output(cmd, text=True).strip()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ENVIRONMENT VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check authentication\n",
    "print(\"\\n1. OpenShift Authentication:\")\n",
    "whoami = run_cmd([\"oc\", \"whoami\"])\n",
    "print(f\"   User: {whoami}\")\n",
    "\n",
    "# Check Ray cluster exists\n",
    "print(\"\\n2. Ray Cluster Status:\")\n",
    "try:\n",
    "    rc_json = run_cmd([\"oc\", \"get\", \"raycluster\", CLUSTER_NAME, \"-n\", NS, \"-o\", \"json\"])\n",
    "    rc_data = json.loads(rc_json)\n",
    "    \n",
    "    state = rc_data.get(\"status\", {}).get(\"state\", \"unknown\")\n",
    "    ready_workers = rc_data.get(\"status\", {}).get(\"availableWorkerReplicas\", 0)\n",
    "    desired_workers = rc_data.get(\"status\", {}).get(\"desiredWorkerReplicas\", 0)\n",
    "    \n",
    "    print(f\"   Cluster: {CLUSTER_NAME}\")\n",
    "    print(f\"   State: {state}\")\n",
    "    print(f\"   Workers: {ready_workers}/{desired_workers}\")\n",
    "    \n",
    "    if state != \"ready\":\n",
    "        print(\"   ‚ö†Ô∏è  WARNING: Cluster not ready yet. Wait a few minutes.\")\n",
    "    \n",
    "    # Check GPU tolerations\n",
    "    head_tolerations = rc_data[\"spec\"][\"headGroupSpec\"][\"template\"][\"spec\"].get(\"tolerations\", [])\n",
    "    has_gpu_tol = any(\n",
    "        t.get(\"key\") == \"nvidia.com/gpu\" and t.get(\"effect\") == \"NoSchedule\"\n",
    "        for t in head_tolerations\n",
    "    )\n",
    "    print(f\"   GPU Toleration: {has_gpu_tol}\")\n",
    "    \n",
    "except subprocess.CalledProcessError:\n",
    "    print(f\"   ‚ùå ERROR: RayCluster '{CLUSTER_NAME}' not found!\")\n",
    "    print(f\"   ‚Üí Apply: oc apply -f 02_ray_localqueue_and_cluster.yaml\")\n",
    "    raise\n",
    "\n",
    "# Check Ray pods\n",
    "print(\"\\n3. Ray Pods:\")\n",
    "try:\n",
    "    pods_output = run_cmd([\"oc\", \"get\", \"pods\", \"-n\", NS, \"-l\", \"ray.io/cluster=ray\", \"--no-headers\"])\n",
    "    if pods_output:\n",
    "        pods = pods_output.split(\"\\n\")\n",
    "        print(f\"   Found {len(pods)} pod(s):\")\n",
    "        for pod in pods:\n",
    "            parts = pod.split()\n",
    "            if len(parts) >= 3:\n",
    "                print(f\"     - {parts[0]}: {parts[2]}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No Ray pods found (cluster may be suspended)\")\n",
    "except:\n",
    "    print(\"   ‚ö†Ô∏è  Could not retrieve pod status\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Prerequisites verified!\")\n",
    "print(\"=\" * 70)\n",
    "## 2Ô∏è‚É£ Install Dependencies\n",
    "\n",
    "Install required Python packages for job submission.\n",
    "# Install packages needed for this notebook\n",
    "!{sys.executable} -m pip install --quiet --upgrade \\\n",
    "    \"numpy==1.26.4\" \\\n",
    "    \"pyarrow==15.0.2\" \\\n",
    "    \"datasets==2.18.0\"\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")\n",
    "## 3Ô∏è‚É£ Authenticate to OpenShift API\n",
    "\n",
    "Required for CodeFlare SDK (even though we don't create cluster from notebook).\n",
    "from codeflare_sdk import TokenAuthentication\n",
    "\n",
    "# Get token and server from oc CLI\n",
    "token = subprocess.check_output([\"oc\", \"whoami\", \"-t\"]).decode().strip()\n",
    "server = subprocess.check_output([\"oc\", \"whoami\", \"--show-server=true\"]).decode().strip()\n",
    "\n",
    "# Authenticate\n",
    "auth = TokenAuthentication(\n",
    "    token=token,\n",
    "    server=server,\n",
    "    skip_tls=True  # Set False if your cluster has proper TLS certs\n",
    ")\n",
    "auth.login()\n",
    "\n",
    "print(f\"‚úÖ Authenticated to: {server}\")\n",
    "## 4Ô∏è‚É£ Connect to Ray Cluster\n",
    "\n",
    "Connect to the existing Ray cluster (created from YAML files).\n",
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "# Ray dashboard URL (internal cluster service)\n",
    "ray_dashboard_url = f\"http://ray-head-svc.{NS}.svc.cluster.local:8265\"\n",
    "\n",
    "print(\"Connecting to Ray cluster...\")\n",
    "print(f\"  Cluster: {CLUSTER_NAME}\")\n",
    "print(f\"  Namespace: {NS}\")\n",
    "print(f\"  Dashboard: {ray_dashboard_url}\")\n",
    "\n",
    "# Create job submission client\n",
    "client = JobSubmissionClient(ray_dashboard_url)\n",
    "\n",
    "# Verify connection by listing existing jobs\n",
    "try:\n",
    "    existing_jobs = client.list_jobs()\n",
    "    print(f\"\\n‚úÖ Connected! Found {len(existing_jobs)} existing job(s).\")\n",
    "    \n",
    "    if existing_jobs:\n",
    "        print(\"\\nExisting jobs:\")\n",
    "        for job_id in existing_jobs:\n",
    "            status = client.get_job_status(job_id)\n",
    "            print(f\"  - {job_id}: {status}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Check Ray cluster is running: oc get pods -n\", NS)\n",
    "    print(\"   2. Check service exists: oc get svc ray-head-svc -n\", NS)\n",
    "    print(\"   3. Wait a few minutes if cluster just started\")\n",
    "    raise\n",
    "## 5Ô∏è‚É£ Configure Training Parameters\n",
    "\n",
    "Set up the training configuration and storage.\n",
    "import os\n",
    "\n",
    "# Storage configuration\n",
    "# Use local persistent storage in the notebook pod\n",
    "STORAGE_PATH = \"/opt/app-root/src\"\n",
    "\n",
    "# Training parameters\n",
    "TRAINING_CONFIG = {\n",
    "    \"model_name\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "    \"use_lora\": True,\n",
    "    \"num_devices\": 2,  # 2 GPUs (head + 1 worker)\n",
    "    \"num_epochs\": 1,\n",
    "    # \"max_steps\": 5,  # Quick test - increase for full training\n",
    "    \"batch_size_per_device\": 1,\n",
    "    \"eval_batch_size_per_device\": 1,\n",
    "    \"storage_path\": f\"{STORAGE_PATH}/ray_finetune_llm_deepspeed/\",\n",
    "    \"ds_config\": \"./deepspeed_configs/zero_3_offload_optim_param.json\"\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìÅ Storage: {STORAGE_PATH}\")\n",
    "print(f\"üìä Expected training time: ~10-15 minutes ({TRAINING_CONFIG['max_steps']} steps)\")\n",
    "print(f\"üí° For full training, increase max_steps to 100+\")\n",
    "## 6Ô∏è‚É£ Prepare Runtime Environment\n",
    "\n",
    "Create requirements file for the Ray job.\n",
    "# Create requirements.txt for Ray runtime environment\n",
    "requirements_content = \"\"\"torch>=2.0.0\n",
    "transformers>=4.30.0\n",
    "datasets>=2.18.0\n",
    "accelerate>=0.20.0\n",
    "deepspeed>=0.9.0\n",
    "peft>=0.4.0\n",
    "bitsandbytes>=0.39.0\n",
    "scipy\n",
    "\"\"\"\n",
    "\n",
    "# Write requirements.txt\n",
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(\"‚úÖ Created requirements.txt:\")\n",
    "print(requirements_content)\n",
    "## 7Ô∏è‚É£ Submit Training Job to Ray\n",
    "\n",
    "Submit the distributed training job to the Ray cluster.\n",
    "\n",
    "**Expected behavior:**\n",
    "- Job submission takes ~1-2 minutes\n",
    "- Training starts on GPU workers\n",
    "- Progress can be monitored in next cell\n",
    "print(\"Submitting training job to Ray cluster...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build entrypoint command\n",
    "entrypoint_cmd = (\n",
    "    f\"python ray_finetune_llm_deepspeed.py \"\n",
    "    f\"--model-name={TRAINING_CONFIG['model_name']} \"\n",
    "    f\"{'--lora ' if TRAINING_CONFIG['use_lora'] else ''}\"\n",
    "    f\"--num-devices={TRAINING_CONFIG['num_devices']} \"\n",
    "    f\"--num-epochs={TRAINING_CONFIG['num_epochs']} \"\n",
    "    f\"--max-steps={TRAINING_CONFIG['max_steps']} \"\n",
    "    f\"--ds-config={TRAINING_CONFIG['ds_config']} \"\n",
    "    f\"--storage-path={TRAINING_CONFIG['storage_path']} \"\n",
    "    f\"--batch-size-per-device={TRAINING_CONFIG['batch_size_per_device']} \"\n",
    "    f\"--eval-batch-size-per-device={TRAINING_CONFIG['eval_batch_size_per_device']}\"\n",
    ")\n",
    "\n",
    "print(\"Entrypoint command:\")\n",
    "print(f\"  {entrypoint_cmd}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Runtime environment configuration\n",
    "runtime_env = {\n",
    "    \"env_vars\": {\n",
    "        \"HF_HOME\": f\"{STORAGE_PATH}/.cache\",\n",
    "        \"TRANSFORMERS_CACHE\": f\"{STORAGE_PATH}/.cache/transformers\",\n",
    "    },\n",
    "    \"pip\": \"requirements.txt\",\n",
    "    \"working_dir\": \"./\",\n",
    "    \"excludes\": [\"/docs/\", \"*.ipynb\", \"*.md\", \".git/\", \"__pycache__/\"]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Submit the job\n",
    "    submission_id = client.submit_job(\n",
    "        entrypoint=entrypoint_cmd,\n",
    "        runtime_env=runtime_env\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Job submitted successfully!\")\n",
    "    print(f\"\\nüìã Job ID: {submission_id}\")\n",
    "    print(f\"\\n‚è±Ô∏è  Initial status: {client.get_job_status(submission_id)}\")\n",
    "    print(f\"\\nüí° Monitor progress in the next cell\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Job submission failed!\")\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(f\"\\nüí° Common issues:\")\n",
    "    print(\"   - Missing training script: ray_finetune_llm_deepspeed.py\")\n",
    "    print(\"   - Missing DeepSpeed config: deepspeed_configs/zero_3_offload_optim_param.json\")\n",
    "    print(\"   - Ray cluster not ready (check previous cells)\")\n",
    "    raise\n",
    "## 8Ô∏è‚É£ Monitor Training Progress\n",
    "\n",
    "Monitor job status and training logs in real-time.\n",
    "\n",
    "**What to look for:**\n",
    "- Job status: PENDING ‚Üí RUNNING ‚Üí SUCCEEDED\n",
    "- Training logs showing epoch/step progress\n",
    "- Loss decreasing over time\n",
    "\n",
    "**Expected timeline:**\n",
    "- Setup: 2-5 minutes (downloading model, installing packages)\n",
    "- Training: 10-15 minutes (5 steps with test config)\n",
    "- Cleanup: 1-2 minutes\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"Monitoring training job...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Job ID: {submission_id}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prev_log_length = 0\n",
    "check_interval = 30  # Check every 30 seconds\n",
    "max_checks = 120  # Max 60 minutes (120 * 30 seconds)\n",
    "\n",
    "for iteration in range(max_checks):\n",
    "    try:\n",
    "        # Get current status\n",
    "        status = client.get_job_status(submission_id)\n",
    "        \n",
    "        # Get logs\n",
    "        logs = client.get_job_logs(submission_id)\n",
    "        lines = logs.split('\\n') if logs else []\n",
    "        \n",
    "        # Show new log lines\n",
    "        if len(lines) > prev_log_length:\n",
    "            new_lines = lines[prev_log_length:]\n",
    "            \n",
    "            # Filter for important lines\n",
    "            important_keywords = [\n",
    "                'step', 'epoch', 'loss', 'loading', 'downloading',\n",
    "                'error', 'training', 'started', 'completed', 'saving'\n",
    "            ]\n",
    "            \n",
    "            for line in new_lines:\n",
    "                line_lower = line.lower()\n",
    "                if any(keyword in line_lower for keyword in important_keywords):\n",
    "                    print(line)\n",
    "            \n",
    "            prev_log_length = len(lines)\n",
    "        \n",
    "        # Check if job finished\n",
    "        if status in [\"SUCCEEDED\", \"FAILED\", \"STOPPED\"]:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(f\"‚úÖ Job finished with status: {status}\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            if status == \"SUCCEEDED\":\n",
    "                print(\"\\nüéâ Training completed successfully!\")\n",
    "                print(f\"\\nüìÅ Model saved to: {TRAINING_CONFIG['storage_path']}\")\n",
    "                print(\"\\n‚úÖ Proceed to next cell to check results\")\n",
    "            elif status == \"FAILED\":\n",
    "                print(\"\\n‚ùå Training failed!\")\n",
    "                print(\"\\nüìã Check the full logs above for error details\")\n",
    "                print(\"\\nüí° Common issues:\")\n",
    "                print(\"   - Out of GPU memory (reduce batch size)\")\n",
    "                print(\"   - Model download failed (check network/auth)\")\n",
    "                print(\"   - Missing dependencies (check requirements.txt)\")\n",
    "            break\n",
    "        \n",
    "        # Show periodic status update\n",
    "        if iteration % 10 == 0:  # Every 5 minutes\n",
    "            elapsed = iteration * check_interval\n",
    "            print(f\"\\n[{elapsed}s elapsed] Status: {status} - Still monitoring...\")\n",
    "        \n",
    "        time.sleep(check_interval)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è  Monitoring interrupted by user\")\n",
    "        print(f\"\\nJob is still running. Current status: {client.get_job_status(submission_id)}\")\n",
    "        print(f\"\\nTo resume monitoring, re-run this cell\")\n",
    "        print(f\"To stop the job, run: client.stop_job('{submission_id}')\")\n",
    "        break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error monitoring job: {e}\")\n",
    "        break\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Monitoring timeout reached\")\n",
    "    print(f\"Job status: {client.get_job_status(submission_id)}\")\n",
    "    print(f\"\\nThe job may still be running. Check Ray dashboard or re-run this cell.\")\n",
    "## 9Ô∏è‚É£ Check Training Results\n",
    "\n",
    "Verify the trained model was saved successfully.\n",
    "import os\n",
    "\n",
    "print(\"Checking training results...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get final job status\n",
    "final_status = client.get_job_status(submission_id)\n",
    "print(f\"Final job status: {final_status}\\n\")\n",
    "\n",
    "# Check if output directory exists\n",
    "output_dir = TRAINING_CONFIG['storage_path']\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"‚úÖ Output directory exists: {output_dir}\\n\")\n",
    "    \n",
    "    # List files in output directory\n",
    "    print(\"Files in output directory:\")\n",
    "    try:\n",
    "        for root, dirs, files in os.walk(output_dir):\n",
    "            level = root.replace(output_dir, '').count(os.sep)\n",
    "            indent = ' ' * 2 * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            subindent = ' ' * 2 * (level + 1)\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                size = os.path.getsize(file_path)\n",
    "                size_mb = size / (1024 * 1024)\n",
    "                print(f\"{subindent}{file} ({size_mb:.2f} MB)\")\n",
    "                if level > 2:  # Limit depth\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"  Error listing files: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå Output directory not found: {output_dir}\")\n",
    "    print(\"\\nThis could mean:\")\n",
    "    print(\"  - Training hasn't completed yet\")\n",
    "    print(\"  - Training failed before saving\")\n",
    "    print(\"  - Storage path is incorrect\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Show full logs if job failed\n",
    "if final_status == \"FAILED\":\n",
    "    print(\"\\nüìã Full job logs:\")\n",
    "    print(\"=\" * 70)\n",
    "    full_logs = client.get_job_logs(submission_id)\n",
    "    print(full_logs[-5000:])  # Last 5000 characters\n",
    "    print(\"=\" * 70)\n",
    "## üîü Optional: Stop Running Job\n",
    "\n",
    "Use this cell if you need to stop a running job.\n",
    "# Uncomment to stop the job\n",
    "# client.stop_job(submission_id)\n",
    "# print(f\"‚úÖ Job {submission_id} stopped\")\n",
    "\n",
    "print(\"üí° Uncomment the lines above to stop the job\")\n",
    "## 1Ô∏è‚É£1Ô∏è‚É£ Optional: View Ray Dashboard\n",
    "\n",
    "Get the URL to the Ray dashboard for detailed monitoring.\n",
    "print(\"Getting Ray Dashboard URL...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Try to get the route\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        \"oc\", \"get\", \"route\", \"-n\", NS,\n",
    "        \"-o\", \"jsonpath={.items[?(@.spec.to.name=='ray-head-svc')].spec.host}\"\n",
    "    ], capture_output=True, text=True, check=False)\n",
    "    \n",
    "    route_host = result.stdout.strip()\n",
    "    \n",
    "    if route_host:\n",
    "        dashboard_url = f\"https://{route_host}/#/overview\"\n",
    "        print(f\"‚úÖ Ray Dashboard URL:\\n\\n   {dashboard_url}\\n\")\n",
    "        print(\"üìä Dashboard shows:\")\n",
    "        print(\"   ‚Ä¢ Cluster status\")\n",
    "        print(\"   ‚Ä¢ Running jobs\")\n",
    "        print(\"   ‚Ä¢ GPU utilization\")\n",
    "        print(\"   ‚Ä¢ Worker nodes\")\n",
    "        print(\"   ‚Ä¢ Logs and metrics\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No route found for Ray dashboard\")\n",
    "        print(\"\\nTo create one:\")\n",
    "        print(f\"   oc expose svc ray-head-svc -n {NS} --port=dashboard\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error getting route: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "## 1Ô∏è‚É£2Ô∏è‚É£ Summary & Next Steps\n",
    "\n",
    "**What you accomplished:**\n",
    "- ‚úÖ Verified Ray cluster prerequisites\n",
    "- ‚úÖ Connected to Ray cluster\n",
    "- ‚úÖ Submitted distributed training job\n",
    "- ‚úÖ Monitored training progress\n",
    "- ‚úÖ Retrieved training results\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. **Evaluate the Model:**\n",
    "   - Load the fine-tuned model\n",
    "   - Test on evaluation dataset\n",
    "   - Compare with base model\n",
    "\n",
    "2. **Deploy the Model:**\n",
    "   - Package model for serving\n",
    "   - Deploy with KServe (Module 05)\n",
    "   - Create inference API\n",
    "\n",
    "3. **Iterate:**\n",
    "   - Adjust hyperparameters\n",
    "   - Train for more epochs\n",
    "   - Try different LoRA configs\n",
    "\n",
    "**Resources:**\n",
    "- Ray Documentation: https://docs.ray.io/\n",
    "- DeepSpeed: https://www.deepspeed.ai/\n",
    "- Llama 3: https://ai.meta.com/llama/\n",
    "\n",
    "## üßπ Cleanup (Optional)\n",
    "\n",
    "**Note:** The Ray cluster is managed by YAML files and should NOT be deleted from this notebook.\n",
    "\n",
    "To delete the Ray cluster, run this command in a terminal:\n",
    "```bash\n",
    "oc delete raycluster ray -n ray-finetune-llm-deepspeed002\n",
    "```\n",
    "\n",
    "Or delete all resources:\n",
    "```bash\n",
    "oc delete -f 02_ray_localqueue_and_cluster.yaml\n",
    "oc delete -f 01_gpu_flavor_and_queue.yaml\n",
    "oc delete -f 03_rbac_notebook_ray.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ccda92-a0f0-4845-a13c-6aa735e75d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f4dde-0637-434c-b501-692570f683cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "# Match what we set in the previous cell\n",
    "namespace = \"ray-finetune-llm-deepspeed002\"\n",
    "cluster_name = \"ray\"\n",
    "\n",
    "ray_url = f\"http://ray-head-svc.{namespace}.svc.cluster.local:8265\"\n",
    "\n",
    "print(f\"Cluster:  {cluster_name}\")\n",
    "print(f\"Namespace:{namespace}\")\n",
    "print(f\"Ray URL:  {ray_url}\")\n",
    "\n",
    "client = JobSubmissionClient(ray_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa2749-e9e1-4ede-852e-019931fdd1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "# Use cluster config (you already have this from earlier cells)\n",
    "namespace = cluster.config.namespace\n",
    "cluster_name = cluster.config.name\n",
    "\n",
    "# Construct URL dynamically\n",
    "ray_url = f\"http://ray-head-svc.{namespace}.svc.cluster.local:8265\"\n",
    "\n",
    "print(f\"Cluster: {cluster_name}\")\n",
    "print(f\"Namespace: {namespace}\")\n",
    "print(f\"Ray URL: {ray_url}\")\n",
    "\n",
    "# Create client\n",
    "client = JobSubmissionClient(ray_url)\n",
    "print(\"‚úì Client connected!\")\n",
    "\n",
    "# Verify\n",
    "jobs = client.list_jobs()\n",
    "print(f\"‚úì Found {len(jobs)} existing jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f3337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage configuration\n",
    "storage_path = '/opt/app-root/src'\n",
    "\n",
    "# The S3 bucket where to store checkpoint.\n",
    "# It can be set manually, otherwise it's retrieved from configured the data connection.\n",
    "s3_bucket = ''  # Empty string for local storage\n",
    "\n",
    "# Comment out S3 logic - keep it simple\n",
    "# if not s3_bucket:\n",
    "#     s3_bucket = os.environ.get('AWS_S3_BUCKET')\n",
    "# if s3_bucket:\n",
    "#     storage_path = f's3://{s3_bucket}'\n",
    "\n",
    "print(f\"Using local storage: {storage_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2be5d8-66c7-46e2-ba3b-fa2f8a03b27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Submit Ray job\n",
    "submission_id = client.submit_job(\n",
    "    entrypoint=\"python mlforeng/llm_finetune/ray_finetune_llm_deepspeed\"\n",
    "               \"--model-name=meta-llama/Meta-Llama-3.1-8B \"\n",
    "               \"--lora \"\n",
    "               \"--num-devices=2 \"\n",
    "               \"--num-epochs=1 \"\n",
    "               \"--max-steps=5 \"\n",
    "               \"--ds-config=./deepspeed_configs/zero_3_offload_optim_param.json \"\n",
    "               f\"--storage-path={storage_path}/ray_finetune_llm_deepspeed/ \"\n",
    "               \"--batch-size-per-device=1 \"\n",
    "               \"--eval-batch-size-per-device=1 \",\n",
    "    runtime_env={\n",
    "        \"env_vars\": {\n",
    "            # Set the following variables if using AWS S3 as storage\n",
    "            # 'AWS_ACCESS_KEY_ID': os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "            # 'AWS_SECRET_ACCESS_KEY': os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "            # 'AWS_DEFAULT_REGION': os.environ.get('AWS_DEFAULT_REGION'),\n",
    "            'HF_HOME': f'{storage_path}/.cache'\n",
    "        },\n",
    "        'pip': 'requirements.txt',\n",
    "        'working_dir': './',\n",
    "        \"excludes\": [\"/docs/\", \"*.ipynb\", \"*.md\"]\n",
    "    },\n",
    ")\n",
    "print(submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdfd0f-9284-42ed-8d0c-623910593cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Monitoring training progress...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "prev_log_length = 0\n",
    "for i in range(60):  # Check for 60 iterations (30 minutes)\n",
    "    logs = client.get_job_logs(submission_id)\n",
    "    lines = logs.split('\\n')\n",
    "    \n",
    "    # Only show new lines\n",
    "    if len(lines) > prev_log_length:\n",
    "        new_lines = lines[prev_log_length:]\n",
    "        for line in new_lines:\n",
    "            if any(keyword in line.lower() for keyword in ['step', 'epoch', 'loss', 'loading', 'error', 'training']):\n",
    "                print(line)\n",
    "        prev_log_length = len(lines)\n",
    "    \n",
    "    status = client.get_job_status(submission_id)\n",
    "    if status in [\"SUCCEEDED\", \"FAILED\", \"STOPPED\"]:\n",
    "        print(f\"\\n‚úì Job finished with status: {status}\")\n",
    "        break\n",
    "    \n",
    "    time.sleep(30)  # Check every 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476f19b-1d51-44f5-8889-c5b01ed36343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.stop_job(submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456f161-5122-4057-a5ac-f7f6b38651ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cluster.down()\n",
    "# We no longer tear down the RayCluster from the notebook.\n",
    "# The RayCluster \"ray\" is managed via YAML and can be deleted by an admin with:\n",
    "#   oc delete raycluster ray -n ray-finetune-llm-deepspeed002\n",
    "print(\"Skipping cluster.down(); RayCluster is managed by YAML.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c7673e-ee7e-47df-804a-9fa3c3b1d492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
