{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "762f3e04",
   "metadata": {},
   "source": [
    "We’re on OpenShift AI workbench.\n",
    "\n",
    "We’ll fine-tune Llama 3.1 8B Instruct using Ray (as in the Red Hat article).\n",
    "\n",
    "This notebook is independent: you don’t need modules 1–4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a40637",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Figure out project root (works in container or local)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR\n",
    "if NOTEBOOK_DIR.name == \"05_llama3_openshift_ai\":\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR.parents[2]  # .../MLforEng\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from mlforeng.models import get_model_spec\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LlamaFineTuneConfig:\n",
    "    model_name: str = \"llama3_8b_instruct\"\n",
    "    dataset_uri: str = \"s3://mybucket/llama3-dataset.jsonl\"  # example\n",
    "    output_dir: str = \"/mnt/models/llama3-finetuned\"        # PVC or S3 path\n",
    "    num_gpus: int = 4\n",
    "    num_epochs: int = 3\n",
    "    per_device_batch_size: int = 4\n",
    "\n",
    "\n",
    "cfg = LlamaFineTuneConfig()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425db85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spec = get_model_spec(cfg.model_name)\n",
    "spec, spec.hf_model_id, spec.extra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1cdf8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "RUN_ON_OPENSHIFT = os.getenv(\"RUN_ON_OPENSHIFT_AI\", \"false\").lower() == \"true\"\n",
    "\n",
    "try:\n",
    "    import ray\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from datasets import load_dataset\n",
    "    HAVE_LLM_STACK = True\n",
    "    LLM_IMPORT_ERROR = None\n",
    "except Exception as e:\n",
    "    HAVE_LLM_STACK = False\n",
    "    LLM_IMPORT_ERROR = e\n",
    "\n",
    "print(\"RUN_ON_OPENSHIFT:\", RUN_ON_OPENSHIFT)\n",
    "print(\"HAVE_LLM_STACK:\", HAVE_LLM_STACK)\n",
    "if not HAVE_LLM_STACK:\n",
    "    print(\"LLM stack not available here. Import error:\", repr(LLM_IMPORT_ERROR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297aa4f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def dry_run_llama_pipeline(cfg: LlamaFineTuneConfig):\n",
    "    print(\"DRY RUN ONLY (no Llama, no Ray).\")\n",
    "    print(\"If this was OpenShift AI, we would:\")\n",
    "    print(f\"- spin up Ray cluster\")\n",
    "    print(f\"- load HF model: {get_model_spec(cfg.model_name).hf_model_id}\")\n",
    "    print(f\"- fine-tune on dataset: {cfg.dataset_uri}\")\n",
    "    print(f\"- save to: {cfg.output_dir}\")\n",
    "\n",
    "dry_run_llama_pipeline(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb167d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not RUN_ON_OPENSHIFT:\n",
    "    print(\"Not running on OpenShift AI (RUN_ON_OPENSHIFT_AI != 'true'). Skipping real fine-tuning.\")\n",
    "elif not HAVE_LLM_STACK:\n",
    "    print(\"LLM stack (ray/transformers/datasets) not available. Skipping real fine-tuning.\")\n",
    "else:\n",
    "    print(\"OK: running real Llama 3.1 fine-tuning on OpenShift AI...\")\n",
    "    import time\n",
    "\n",
    "    # === This is where you paste/adapt the Ray + HF code from the article ===\n",
    "    # Example sketch (NOT full code):\n",
    "\n",
    "    ray.init(address=\"auto\")\n",
    "    print(\"Ray cluster resources:\", ray.cluster_resources())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(spec.hf_model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(spec.hf_model_id)\n",
    "\n",
    "    dataset = load_dataset(\"json\", data_files={\"train\": cfg.dataset_uri})\n",
    "\n",
    "    # ... tokenization, collator, Ray Trainer or HF Trainer setup ...\n",
    "    # trainer = TransformersTrainer(...)\n",
    "    # result = trainer.fit()\n",
    "\n",
    "    # Simulate long run for demo\n",
    "    time.sleep(1)\n",
    "    print(\"Fine-tuning completed (placeholder). Checkpoints saved to:\", cfg.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0bab1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from mlforeng.registry import register_external_model  # you create this\n",
    "\n",
    "logical_name = \"llama3_8b_instruct_supportbot\"\n",
    "\n",
    "if RUN_ON_OPENSHIFT and HAVE_LLM_STACK:\n",
    "    # Real path where trainer saved the checkpoint\n",
    "    model_path = cfg.output_dir\n",
    "else:\n",
    "    # Placeholder for local dev so the code path is testable\n",
    "    model_path = \"/mnt/models/llama3-finetuned-FAKE\"\n",
    "\n",
    "print(\"Registering logical model name:\", logical_name, \"->\", model_path)\n",
    "registered_path = register_external_model(logical_name, model_path)\n",
    "registered_path\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
